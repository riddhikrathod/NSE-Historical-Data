{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES & SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "# ! pip -q install nselib\n",
    "# ! pip -q install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import warnings\n",
    "import yfinance\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from nselib import capital_market\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Riddhik\\OneDrive\\Desktop\\Personal\\Financial Planning\\Data\n"
     ]
    }
   ],
   "source": [
    "# Set Current Directory\n",
    "os.chdir('C://Users//Riddhik//OneDrive//Desktop//Personal//Financial Planning//Data//')\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's Date: 2023/07/05\n"
     ]
    }
   ],
   "source": [
    "# Get Today's Date\n",
    "dt_today = str(dt.datetime.today())\n",
    "curr_year = dt_today[:4]\n",
    "curr_month = dt_today[5:7]\n",
    "curr_date = dt_today[8:10]\n",
    "save_date =  curr_year + \"_\" + curr_month + \"_\" + curr_date\n",
    "print(\"Today's Date: \" + curr_year + \"/\" + curr_month + \"/\" + curr_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pull start date: 1997-01-01\n",
      "Data pull end date:  2023-07-05\n"
     ]
    }
   ],
   "source": [
    "# Date Manipulation\n",
    "START_YEAR = 1997; START_MONTH = 1; START_DAY = 1\n",
    "END_YEAR = int(curr_year); END_MONTH = int(curr_month); END_DAY = int(curr_date)\n",
    "final_start_date = dt.date(START_YEAR, START_MONTH, START_DAY)\n",
    "final_end_date = dt.date(END_YEAR, END_MONTH, END_DAY)\n",
    "print(\"Data pull start date:\", final_start_date)\n",
    "print(\"Data pull end date: \", final_end_date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ INPUT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Index Input File\n",
    "input_df = pd.read_csv('./input_file.csv')\n",
    "nse_py_data = input_df[input_df['EXTRACT_DATA'] == 1]\n",
    "y_fin_data = input_df[input_df['EXTRACT_DATA'] == 2]\n",
    "del input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Files nse_py:  38\n",
      "Count of Files yfin:  6\n"
     ]
    }
   ],
   "source": [
    "# Show Counts\n",
    "print(\"Count of Files nse_py: \", len(nse_py_data['INDEX'].to_list()))\n",
    "print(\"Count of Files yfin: \", len(y_fin_data['INDEX'].to_list()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skeleton(final_start_date, final_end_date):\n",
    "    '''Create Weekly Skeleton'''\n",
    "    \n",
    "    skeleton = pd.DataFrame(pd.date_range(final_start_date, final_end_date, freq='W-FRI'))\n",
    "    skeleton.columns = ['Date']\n",
    "    \n",
    "    return skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_impute(skeleton_df, csv_dataset, index_close_df, day):\n",
    "    '''Create proxy dates for missing values for imputing Close values'''\n",
    "    \n",
    "    null_df_index = csv_dataset[csv_dataset['Close'].isnull()]\n",
    "    null_df_skeleton = skeleton_df[skeleton_df['Date'].isin(null_df_index.Date)]\n",
    "    null_df_skeleton['Proxy_Date'] = null_df_skeleton['Date'] - dt.timedelta(days = day)\n",
    "    imputed_df = pd.merge(null_df_skeleton, index_close_df, left_on = 'Proxy_Date',\n",
    "                          right_on = 'Date', how = 'left')\n",
    "    imputed_df.drop(['Proxy_Date', 'Date_y'], axis = 1, inplace = True)  \n",
    "    imputed_df.columns = ['Date', 'Close']\n",
    "    \n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(skeleton_df, csv_dataset, index_close_df):\n",
    "    '''Impute for 4 days from Friday if Close value not found'''\n",
    "    \n",
    "    # create impute data upto 4 previous close values\n",
    "    impute_1 = rec_impute(skeleton_df, csv_dataset, index_close_df, 1)\n",
    "    impute_2 = rec_impute(skeleton_df, impute_1, index_close_df, 2)\n",
    "    impute_3 = rec_impute(skeleton_df, impute_2, index_close_df, 3)\n",
    "    impute_4 = rec_impute(skeleton_df, impute_3, index_close_df, 4)\n",
    "    impute_1.dropna(inplace = True); impute_2.dropna(inplace = True)\n",
    "    impute_3.dropna(inplace = True); impute_4.dropna(inplace = True)\n",
    "    final_impute_df = pd.concat([impute_1, impute_2, impute_3, impute_4])\n",
    "    \n",
    "    # create final inpute dataframe\n",
    "    fin_df = pd.merge(csv_dataset, final_impute_df, on = 'Date', how = 'left')\n",
    "    fin_df.Close_x.fillna(fin_df.Close_y, inplace=True)\n",
    "    fin_df.drop(['Close_y'], axis = 1, inplace = True)\n",
    "    fin_df.columns = ['Date', 'Close']\n",
    "    \n",
    "    return fin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_history_nse_py(df, index_name):\n",
    "    '''Return DAILY historical close data of the index from nse'''\n",
    "    \n",
    "    for year in range(1994, int(curr_year)+1):\n",
    "        \n",
    "        from_date_str = '01-01-' + str(year)\n",
    "        to_date_str = str(curr_date) + \"-\" + str(curr_month)  + \"-\" + str(curr_year)\n",
    "        \n",
    "        try:\n",
    "            df = capital_market.index_data(index=index_name, from_date=from_date_str, to_date=to_date_str)\n",
    "            df = df[['TIMESTAMP', 'CLOSE_INDEX_VAL', 'INDEX_NAME']]\n",
    "            break \n",
    "        except:    \n",
    "            pass\n",
    "\n",
    "    df.rename(columns={'TIMESTAMP':'Date', 'CLOSE_INDEX_VAL':'Close', 'INDEX_NAME':'Index'}, inplace=True)\n",
    "    df = df.round({'Close':0})\n",
    "    df.drop_duplicates(subset = ['Date'], keep = 'first', inplace = True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "    df.drop_duplicates(subset = ['Date'], keep = 'first', inplace = True)\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK ETF/TICKER\n",
    "#get_full_history_nse_py(nse_py_data, 'Nifty50 Value 20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_history_yfin(etf_name, start_date, end_date):\n",
    "    '''Return DAILY historical close data of the index from yahoo finance'''\n",
    "    \n",
    "    ticker = yfinance.Ticker(etf_name)\n",
    "    data = ticker.history(interval = '1d', start = start_date,\n",
    "                          end = end_date, actions = False)\n",
    "    data = data.reset_index()\n",
    "    data = data[['Date', 'Close']]\n",
    "    data = data[data['Close'].notna()]\n",
    "    data['Date']= pd.to_datetime(data['Date'])\n",
    "    data = data.round({'Close':0})\n",
    "    data.drop_duplicates(subset = ['Date'], keep = 'first', inplace = True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK ETF/TICKER\n",
    "#get_full_history_yfin('SILVERBEES.NS', '2023-01-01', '2023-02-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weekly_data(dataset, from_start_date, from_end_date):\n",
    "    '''Create Weekly data after pulling daily data'''\n",
    "    \n",
    "    # Create skeleton\n",
    "    skeleton_df = create_skeleton(from_start_date, from_end_date)\n",
    "    \n",
    "    # merge with skeleton\n",
    "    csv_dataset = pd.merge(skeleton_df, dataset, on = 'Date', how = 'left')\n",
    "\n",
    "    # filter non-existent data\n",
    "    csv_dataset = csv_dataset[csv_dataset.Date >= min(dataset.Date)]\n",
    "    csv_dataset = impute_data(skeleton_df, csv_dataset, dataset)\n",
    "\n",
    "    # forward fill missing values that could not be imputed\n",
    "    csv_dataset.fillna(method='ffill', inplace=True)\n",
    "    csv_dataset = csv_dataset[csv_dataset['Close'].notna()]\n",
    "    \n",
    "    return csv_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved daily data: ICICI500.NS\n",
      "Saved weekly data: ICICI500.NS\n",
      "\n",
      "Saved daily data: ICICIMCAP.NS\n",
      "Saved weekly data: ICICIMCAP.NS\n",
      "\n",
      "Saved daily data: GOLDBEES.NS\n",
      "Saved weekly data: GOLDBEES.NS\n",
      "\n",
      "Saved daily data: ICICIB22.NS\n",
      "Saved weekly data: ICICIB22.NS\n",
      "\n",
      "Saved daily data: SILVERBEES.NS\n",
      "Saved weekly data: SILVERBEES.NS\n",
      "\n",
      "Saved daily data: SHARIABEES.NS\n",
      "Saved weekly data: SHARIABEES.NS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Yahoo Finance\n",
    "for etf_name in y_fin_data['ETF'].tolist():\n",
    "    \n",
    "    # Get Daily Data\n",
    "    daily_data = get_full_history_yfin(etf_name, start_date=final_start_date, end_date=final_end_date)\n",
    "    daily_data['Date'] = pd.to_datetime(daily_data['Date']).dt.tz_localize(None)\n",
    "    min_date = str(daily_data['Date'].min())[:10]\n",
    "    max_date = str(daily_data['Date'].max())[:10]\n",
    "    daily_data.to_csv('./daily/' + etf_name + \"_\" + min_date + \"_\" + max_date + \".csv\", index=False)\n",
    "    del min_date, max_date\n",
    "    print(\"Saved daily data: \" + etf_name)\n",
    "    \n",
    "    # Create Weekly Data\n",
    "    data = create_weekly_data(daily_data, final_start_date, final_end_date)\n",
    "    min_date = str(data['Date'].min())[:10]\n",
    "    max_date = str(data['Date'].max())[:10]\n",
    "    data['Index'] = etf_name\n",
    "    \n",
    "    # data cleanup\n",
    "    if etf_name == 'GOLDBEES.NS':\n",
    "        data.loc[data['Date'] == '2019-12-20', 'Close'] = 33.65\n",
    "    if etf_name == 'HNGSNGBEES.NS':\n",
    "        data.loc[data['Date'] == '2019-12-20', 'Close'] = 368.94\n",
    "    if etf_name == 'ICICI500.NS':\n",
    "        data_p1 = data[data['Date'] <= '2021-10-22']\n",
    "        data_p2 = data[data['Date'] >= '2021-10-29']\n",
    "        data_p1['Close'] = data_p1['Close'].div(10)\n",
    "        data = pd.concat([data_p1, data_p2])\n",
    "        data.loc[data['Date'] == '2021-10-29', 'Close'] = 24.99\n",
    "        del data_p2, data_p1\n",
    "    if etf_name == 'ICICIMCAP.NS':\n",
    "        data = data[data['Date'] >= '2019-02-15']\n",
    "    \n",
    "    data.to_csv('./weekly/' + etf_name + \"_\" + min_date + \"_\" + max_date + \".csv\", index=False)\n",
    "    del min_date, max_date\n",
    "    print(\"Saved weekly data: \" + etf_name)\n",
    "    print()\n",
    "\n",
    "del daily_data, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved daily data: NIFTY 100\n",
      "Saved weekly data: NIFTY 100\n",
      "\n",
      "Saved daily data: NIFTY 50\n",
      "Saved weekly data: NIFTY 50\n",
      "\n",
      "Saved daily data: NIFTY LARGEMIDCAP 250\n",
      "Saved weekly data: NIFTY LARGEMIDCAP 250\n",
      "\n",
      "Saved daily data: NIFTY MICROCAP 250\n",
      "Saved weekly data: NIFTY MICROCAP 250\n",
      "\n",
      "Saved daily data: NIFTY MIDCAP 100\n",
      "Saved weekly data: NIFTY MIDCAP 100\n",
      "\n",
      "Saved daily data: NIFTY MIDCAP 150\n",
      "Saved weekly data: NIFTY MIDCAP 150\n",
      "\n",
      "Saved daily data: NIFTY MIDCAP 50\n",
      "Saved weekly data: NIFTY MIDCAP 50\n",
      "\n",
      "Saved daily data: NIFTY NEXT 50\n",
      "Saved weekly data: NIFTY NEXT 50\n",
      "\n",
      "Saved daily data: NIFTY SMALLCAP 250\n",
      "Saved weekly data: NIFTY SMALLCAP 250\n",
      "\n",
      "Saved daily data: NIFTY SMALLCAP 50\n",
      "Saved weekly data: NIFTY SMALLCAP 50\n",
      "\n",
      "Saved daily data: NIFTY AUTO\n",
      "Saved weekly data: NIFTY AUTO\n",
      "\n",
      "Saved daily data: NIFTY BANK\n",
      "Saved weekly data: NIFTY BANK\n",
      "\n",
      "Saved daily data: NIFTY FINANCIAL SERVICES\n",
      "Saved weekly data: NIFTY FINANCIAL SERVICES\n",
      "\n",
      "Saved daily data: NIFTY FMCG\n",
      "Saved weekly data: NIFTY FMCG\n",
      "\n",
      "Saved daily data: NIFTY HEALTHCARE INDEX\n",
      "Saved weekly data: NIFTY HEALTHCARE INDEX\n",
      "\n",
      "Saved daily data: NIFTY IT\n",
      "Saved weekly data: NIFTY IT\n",
      "\n",
      "Saved daily data: NIFTY PHARMA\n",
      "Saved weekly data: NIFTY PHARMA\n",
      "\n",
      "Saved daily data: NIFTY PRIVATE BANK\n",
      "Saved weekly data: NIFTY PRIVATE BANK\n",
      "\n",
      "Saved daily data: NIFTY PSU BANK\n",
      "Saved weekly data: NIFTY PSU BANK\n",
      "\n",
      "Saved daily data: NIFTY200 MOMENTUM 30\n",
      "Saved weekly data: NIFTY200 MOMENTUM 30\n",
      "\n",
      "Saved daily data: NIFTY ALPHA 50\n",
      "Saved weekly data: NIFTY ALPHA 50\n",
      "\n",
      "Saved daily data: NIFTY DIVIDEND OPPORTUNITIES 50\n",
      "Saved weekly data: NIFTY DIVIDEND OPPORTUNITIES 50\n",
      "\n",
      "Saved daily data: NIFTY MIDCAP150 QUALITY 50\n",
      "Saved weekly data: NIFTY MIDCAP150 QUALITY 50\n",
      "\n",
      "Saved daily data: NIFTY100 EQUAL WEIGHT\n",
      "Saved weekly data: NIFTY100 EQUAL WEIGHT\n",
      "\n",
      "Saved daily data: NIFTY100 LOW VOLATILITY 30\n",
      "Saved weekly data: NIFTY100 LOW VOLATILITY 30\n",
      "\n",
      "Saved daily data: NIFTY100 QUALITY 30\n",
      "Saved weekly data: NIFTY100 QUALITY 30\n",
      "\n",
      "Saved daily data: NIFTY200 QUALITY 30\n",
      "Saved weekly data: NIFTY200 QUALITY 30\n",
      "\n",
      "Saved daily data: NIFTY50 EQUAL WEIGHT\n",
      "Saved weekly data: NIFTY50 EQUAL WEIGHT\n",
      "\n",
      "Saved daily data: NIFTY50 VALUE 20\n",
      "Saved weekly data: NIFTY50 VALUE 20\n",
      "\n",
      "Saved daily data: NIFTY COMMODITIES\n",
      "Saved weekly data: NIFTY COMMODITIES\n",
      "\n",
      "Saved daily data: NIFTY CPSE\n",
      "Saved weekly data: NIFTY CPSE\n",
      "\n",
      "Saved daily data: NIFTY INDIA CONSUMPTION\n",
      "Saved weekly data: NIFTY INDIA CONSUMPTION\n",
      "\n",
      "Saved daily data: NIFTY INDIA DIGITAL\n",
      "Saved weekly data: NIFTY INDIA DIGITAL\n",
      "\n",
      "Saved daily data: NIFTY INDIA MANUFACTURING\n",
      "Saved weekly data: NIFTY INDIA MANUFACTURING\n",
      "\n",
      "Saved daily data: NIFTY INFRASTRUCTURE\n",
      "Saved weekly data: NIFTY INFRASTRUCTURE\n",
      "\n",
      "Saved daily data: NIFTY MNC\n",
      "Saved weekly data: NIFTY MNC\n",
      "\n",
      "Saved daily data: NIFTY SERVICES SECTOR\n",
      "Saved weekly data: NIFTY SERVICES SECTOR\n",
      "\n",
      "Saved daily data: NIFTY100 ESG\n",
      "Saved weekly data: NIFTY100 ESG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NSE Data\n",
    "for index_name in nse_py_data['INDEX'].tolist():\n",
    "    \n",
    "    # Get Daily Data\n",
    "    daily_data = get_full_history_nse_py(nse_py_data, index_name)\n",
    "    min_date = str(daily_data['Date'].min())[:10]\n",
    "    max_date = str(daily_data['Date'].max())[:10]\n",
    "    daily_data.to_csv('./daily/' + index_name + \"_\" + min_date + \"_\" + max_date + \".csv\", index=False)\n",
    "    daily_data.drop(columns=['Index'], inplace=True)\n",
    "    print(\"Saved daily data: \" + index_name)\n",
    "    del min_date, max_date\n",
    "    \n",
    "    # Create Weekly Data\n",
    "    data = create_weekly_data(daily_data, final_start_date, final_end_date)\n",
    "    min_date = str(data['Date'].min())[:10]\n",
    "    max_date = str(data['Date'].max())[:10]\n",
    "    data['Index'] = index_name\n",
    "    data.to_csv('./weekly/' + index_name +  \"_\" + min_date + \"_\" + max_date + \".csv\", index=False)\n",
    "    del min_date, max_date\n",
    "    print(\"Saved weekly data: \" + index_name)\n",
    "    print()\n",
    "\n",
    "del daily_data, data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
